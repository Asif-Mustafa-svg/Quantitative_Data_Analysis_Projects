# I work across a broad range of quantitative data analysis, statistical modeling, and machine learning tools to help organizations make evidence-based decisions. My technical foundation includes major statistical software such as SPSS, AMOS for structural equation modeling, and SmartPLS for PLS-SEM. When needed, I also use Stata and Minitab for additional econometric and statistical workflows.

# I am highly proficient in Python and R, which I use for data analysis, modeling, and automation. In Python, I rely on libraries like Pandas, NumPy, SciPy, StatsModels, and Scikit-Learn to perform data processing, statistical tests, and predictive modeling. In R, I use the tidyverse (including dplyr and ggplot2) along with caret for machine learning workflows. For advanced structural equation modeling, I work with lavaan and semPlot in R, PySEM in Python, and JASP for Bayesian modeling and SEM visualization.

# On the machine learning and data engineering side, I use the Scikit-Learn, XGBoost, and LightGBM ecosystem, supported by development environments such as Jupyter Notebook and VS Code. I also work with SQL for structured data management and build dashboards in Tableau, Power BI, and Looker Studio to communicate insights effectively.

# For data collection, I frequently use APIs, web scraping techniques, and survey tools like Qualtrics and Google Forms.

# Overall, my work focuses on turning raw numerical data into clear, reliable insights. I deliver statistical models, forecasts, SEM/CFA results, high-quality visualizations, and automated reports that support strategic decision-making across research, business, and technical environments.
